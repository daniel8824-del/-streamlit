import streamlit as st
import os
from langchain.embeddings.openai import OpenAIEmbeddings  # OpenAI ì„ë² ë”© ëª¨ë¸
from langchain.vectorstores import FAISS  # FAISS ë²¡í„° ì €ì¥ì†Œ
from langchain.chat_models import ChatOpenAI  # OpenAI ì±— ëª¨ë¸
from langchain.chains import ConversationalRetrievalChain  # ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸
from langchain.memory import ConversationBufferMemory  # ëŒ€í™” ê¸°ë¡ ë©”ëª¨ë¦¬
from dotenv import load_dotenv  # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="í˜„ëŒ€ìë™ì°¨ ì„¤ëª…ì„œ ì±—ë´‡",
    page_icon="ğŸš—",
    layout="centered"
)

# ì œëª© ë° ì„¤ëª…
st.title("ğŸš— í˜„ëŒ€ìë™ì°¨ ì„¤ëª…ì„œ ì±—ë´‡")
st.markdown("""
ì´ ì±—ë´‡ì€ í˜„ëŒ€ìë™ì°¨ ì„¤ëª…ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
RAG(Retrieval-Augmented Generation) ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ PDF í˜•ì‹ì˜ ì„¤ëª…ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , 
ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
""")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'chatbot' not in st.session_state:
    st.session_state.chatbot = None

if 'ready' not in st.session_state:
    st.session_state.ready = False

def load_vectorstore():
    """
    ì €ì¥ëœ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Returns:
        FAISS: ë¡œë“œëœ ë²¡í„° ì €ì¥ì†Œ
    """
    # ë²¡í„° ì €ì¥ì†Œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.exists("faiss_index"):
        st.error("ë²¡í„° ì €ì¥ì†Œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. create_vectorstore.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return None
    
    with st.spinner("ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤..."):
        try:
            # OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            embeddings = OpenAIEmbeddings()
            
            # ì €ì¥ëœ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
            vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
            st.success("ë²¡í„° ì €ì¥ì†Œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return vectorstore
        except Exception as e:
            st.error(f"ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None

def create_chatbot(vectorstore):
    """
    í˜„ëŒ€ìë™ì°¨ ì„¤ëª…ì„œ ì±—ë´‡ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Returns:
        ConversationalRetrievalChain: ìƒì„±ëœ ì±—ë´‡ ì²´ì¸
    """
    # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )
    
    # OpenAI ì±— ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(
        model_name="gpt-4o",
        temperature=0.2
    )
    
    # ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ ìƒì„±
    chatbot = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(
            search_kwargs={"k": 3}
        ),
        memory=memory,
        return_source_documents=True,
        output_key="answer"
    )
    
    return chatbot

# ì‚¬ì´ë“œë°”ì— ì±—ë´‡ ì´ˆê¸°í™” ë²„íŠ¼
with st.sidebar:
    st.header("ì±—ë´‡ ì„¤ì •")
    
    if st.button("ì±—ë´‡ ì´ˆê¸°í™”"):
        vectorstore = load_vectorstore()
        if vectorstore:
            with st.spinner("ì±—ë´‡ì„ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤..."):
                st.session_state.chatbot = create_chatbot(vectorstore)
                st.session_state.ready = True
                st.session_state.messages = []
                st.success("ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    st.markdown("---")
    st.markdown("### ì˜ˆì‹œ ì§ˆë¬¸")
    example_questions = [
        "ì•„ë°˜ë–¼ ì—”ì§„ ì˜¤ì¼ì€ ì–´ë–»ê²Œ êµì²´í•˜ë‚˜ìš”?",
        "íƒ€ì´ì–´ ê³µê¸°ì••ì€ ì–¼ë§ˆë¡œ ìœ ì§€í•´ì•¼ í•˜ë‚˜ìš”?",
        "íƒ€ì´ì–´ê°€ í‘í¬ë‚¬ì–´. í•´ê²°ì±…ì„ ì•Œë ¤ì¤˜",
        "ì°½ë¬¸ì— ì„œë¦¬ê°€ ìê¾¸ ê»´"
    ]
    
    for q in example_questions:
        if st.button(q):
            if st.session_state.ready:
                st.session_state.messages.append({"role": "user", "content": q})
                with st.chat_message("user"):
                    st.markdown(q)
            else:
                st.warning("ë¨¼ì € ì±—ë´‡ì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")

# ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    if not st.session_state.ready:
        st.warning("ë¨¼ì € ì±—ë´‡ì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
    else:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ì±—ë´‡ ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # ì±—ë´‡ì— ì§ˆë¬¸ ì „ë‹¬ ë° ì‘ë‹µ ë°›ê¸°
                    response = st.session_state.chatbot.invoke({"question": prompt})
                    
                    # ì°¸ê³  í˜ì´ì§€ ì¶”ì¶œ
                    pages = [doc.metadata.get('page', 'N/A') for doc in response["source_documents"]]
                    
                    # ì‘ë‹µ ë° ì°¸ê³  í˜ì´ì§€ í‘œì‹œ
                    full_response = f"{response['answer']}\n\n**ì°¸ê³  í˜ì´ì§€**: {', '.join(map(str, pages))}"
                    message_placeholder.markdown(full_response)
                    
                    # ì‘ë‹µ ì €ì¥
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                
                except Exception as e:
                    message_placeholder.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ì´ˆê¸° ì•ˆë‚´ ë©”ì‹œì§€
if not st.session_state.messages:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ 'ì±—ë´‡ ì´ˆê¸°í™”' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”.") 